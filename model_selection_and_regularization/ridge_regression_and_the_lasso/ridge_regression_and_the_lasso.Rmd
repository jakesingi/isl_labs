---
title: "ridge_regression_and_the_lasso.Rmd"
output: github_document
---

# Lab 2: Ridge Regression and the Lasso

### Ridge Regression: predicting Salary based on variables in the `Hitters` data set.

```{r}
library(ISLR)
hitters = na.omit(Hitters)
names(hitters)
```

```{r}
# Split data into X and y, this function converts categorical vars to dummy vars
X = model.matrix(Salary~., data=hitters)[ , -1]  # drop intercept column
y = hitters$Salary

# perform ridge regression with glmnet() function, setting alpha=0. alpha=1 performs lasso
library(glmnet)
grid = 10^seq(10, -2, length=100)  # grid of potential lambda values
ridge_mod = glmnet(X, y, alpha=0, lambda=grid)
dim(coef(ridge_mod))  # A matrix for which the rows are predictors, and the columns are values of lambda. The entries are the corresponding coefficient estimates
```

When using ridge regression, we expect higher values of lambda to result in lower coefficient estimates and therefore also a lower l2 norm. Let's check that now.

```{r}
ridge_mod$lambda[60]
coef(ridge_mod)[, 60]
sqrt(sum(coef(ridge_mod)[-1, 60]^2))  # l2 norm

ridge_mod$lambda[2]
coef(ridge_mod)[, 2]
sqrt(sum(coef(ridge_mod)[-1, 2]^2))  # l2 norm
```

Here, we compare a lambda of 705 to a lambda of over 7 billion. The latter, as we can see, results in coefficients that are basically all close to 0, with a small l2 norm. With 705, we see that opposite. So, our intuition checks out.

```{r}
# let's use predict() in order to choose a specific value of lambda
predict(ridge_mod, s=50, type='coefficients')[1:20, ]
```

These coefficients, relative to the ones above, are much bigger because of the smaller value of lambda, which makes sense.

```{r}
# Split data into train and test to be able to estimate test error
set.seed(1)
# Generate indices for training data
train = sample(1:nrow(X), nrow(X)/2)
test = -train
y_test = y[test]

# Model and predict
ridge_mod = glmnet(X[train, ], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge_preds = predict(ridge_mod, s=4, newx=X[test, ])  # setting lambda=4 like the book
mean((y_test - ridge_preds)^2)
```

Does this give us an advantage over OLS? Let's see.

```{r}
ridge_preds = predict(ridge_mod, s=0, newx=X[test, ], exact=T, x=X[train, ], y=y[train])  # setting lambda=0 and exact=T to achieve OLS
mean((y_test - ridge_preds)^2)
```

(We could have just used the `lm()` function to do this, and would've seen useful results too, like p-values and R-sq, but doing it this way is useful to know in this context.)

Now, instead of just randomly choosing lambda = 4, we'll choose it using cross-validation, specifically the `cv.glmnet()` function that performs 10-fold CV by default.

```{r}
set.seed(1)
cv_out = cv.glmnet(X[train, ], y[train], alpha=0)
plot(cv_out)
best_lam = cv_out$lambda.min
best_lam
```

Hence the best value for lambda here is about 212. Let's check its corresponding MSE.

```{r}
# MSE for lambda=212
ridge_preds = predict(ridge_mod, s=best_lam, newx=X[test, ])
mean((y_test - ridge_preds)^2)
```

This is the lowest MSE we've seen so far and therefore our best model! Now, as usual, we'll fit this model to the entire data set to achieve stable coefficient estimates.

```{r}
final_ridge = glmnet(X, y, alpha=0)
predict(final_ridge, type='coefficients', s=best_lam)[1:20, ]  # examine coeffs
```

These coefficients are those of our final and best ridge model!

### The Lasso: do we get better results in this setting?

```{r}
# train and plot lasso
lasso_mod = glmnet(X[train, ], y[train], alpha=1, lambda=grid)
plot(lasso_mod)
```

In this plot, we see that as the L1 Norm increases, which corresponds to decreasing lambda, at different moments some coefficients become non-zero. In other words, for certain choices of lambda, the lasso is zeroing out certain predictors!

As with ridge regression, let's perform cross-validation to find the best value for lambda.

```{r}
set.seed(1)
cv_lasso = cv.glmnet(X[train, ], y[train], alpha=1)
plot(cv_lasso)
best_lam = cv_lasso$lambda.min
best_lam
```

The best lambda value here is about 17. Let's see what we get when we calculate the model's MSE with this value of lambda.

```{r}
# MSE for lambda=17
lasso_preds = predict(lasso_mod, s=best_lam, newx=X[test, ])
mean((y_test - lasso_preds)^2)
```

This test MSE is better than the OLS result, and just slightly more than the ridge result. Because it is only just a little bit higher than the ridge test MSE, we might prefer this lasso model because it encourages sparser models. We can see that below by looking at its coefficients.

```{r}
final_lasso = glmnet(X, y, alpha=1)
predict(final_lasso, type='coefficients', s=best_lam)[1:20, ]  # examine coeffs
```

And so, the lasso has zeroed out 12/19 variables. Because it has created a simpler model than the ridge model, we might prefer it despite its slightly greater test MSE. For example, if inference is something we value, we'd probably prefer this lasso model. But, if we want sheer prediction accuracy, the ridge model might be better. 

Above all, this example and comparison shows some of the cool decisions you might have to make as a data scientist regarding model selection!