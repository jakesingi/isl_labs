---
title: "cross_validation_and_bootstrap.Rmd"
output: github_document
---

# ISL Lab: Cross-Validation and the Bootstrap

### Validation Set Approach

```{r}
library(ISLR)
set.seed(1)
# generate random training data
train = sample(392, 196)

# Use Auto data to fit a linear regression model
lm_fit = lm(mpg~horsepower, data=Auto, subset=train)

# Make predictions and calculate MSE
mean((Auto$mpg - predict(lm_fit, Auto))[-train]^2)
```

So the MSE here is 26.14. Earlier in the book we saw polynomial regression might work better than OLS. Let's try them here...

```{r}
lm_fit2 = lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
print(paste("Second degree polynomial MSE:", mean((Auto$mpg - predict(lm_fit2, Auto))[-train]^2), sep=" "))

lm_fit3 = lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
print(paste("Third degree polynomial MSE:", mean((Auto$mpg - predict(lm_fit3, Auto))[-train]^2), sep=" "))
```

So, these models give a better MSE than what we saw with OLS. Note that in this example we used the syntax `-train` when calculating the MSE to get our validation set. Now, if we generate new data, let's see what happens...

```{r}
# repeating what was done above
set.seed(2)
# generate random training data
train = sample(392, 196)

# Use Auto data to fit a linear regression model
lm_fit = lm(mpg~horsepower, data=Auto, subset=train)

# Make predictions and calculate MSE
print(paste("OLS MSE:", mean((Auto$mpg - predict(lm_fit, Auto))[-train]^2), sep=" "))

lm_fit2 = lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
print(paste("Second degree polynomial MSE:", mean((Auto$mpg - predict(lm_fit2, Auto))[-train]^2), sep=" "))

lm_fit3 = lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
print(paste("Third degree polynomial MSE:", mean((Auto$mpg - predict(lm_fit3, Auto))[-train]^2), sep=" "))
```

Since the data is different, we get slightly different MSEs. This iteration was useful though, since we see that the second degree polynomial regression performs best.

### Leave-One-Out Cross-Validation

```{r}
# LOOCV's cv.glm() is part of the boot library
library(boot)
# simple OLS regression
glm_fit = glm(mpg~horsepower, data=Auto)
# default value for K is nrows(data), so this is effectively LOOCV
cv_error = cv.glm(Auto, glm_fit)
cv_error$delta  # the cross-val estimate of test error
```

Let's repeat this process with increasing degrees of polynomials, up to 5.

```{r}
cv_errors = rep(0, 5)
for (deg in 1:5) {
  model = glm(mpg~poly(horsepower, deg), data=Auto)
  cv_errors[deg] = cv.glm(Auto, model)$delta[1]
}

cv_errors
```

From these results, we see the polynomial degree increase from 1 to 2 had a big effect on the cross-val test error estimate. From degree 2 onwards, we see some improvement, but it seems marginal at best.

### k-Fold Cross Validation

```{r}
# Follow the same for loop approach as above, this time setting K=10
set.seed(3)
cv_errors = rep(0, 10)
for (i in 1:10) {
  model = glm(mpg~poly(horsepower, i), data=Auto)
  cv_errors[i] = cv.glm(Auto, model, K=10)$delta[1]
}

cv_errors
```

We see similar results. The most noticeable thing was the runtime of this chunk was a lot faster than that of LOOCV.

### The Bootstrap

## First, we estimate the accuracy of a statistic of interest

```{r}
# Takes in the data, and a vector indicating which observations will be used to estimate our parameter of interest, alpha. Then returns alpha based on formula on p. 187
alpha_estimator = function(data, index) {
  X = data$X[index]
  Y = data$Y[index]
  alpha = ((var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2*cov(X, Y)))
  return(alpha)
}

set.seed(1)
# Use sample() with the Portfolio data set to select 100 observations from 1:100, with replacement. This is effecitvely our bootstrap data set
alpha_estimator(Portfolio, sample(100, 100, replace=T))
```

This is one estimate for alpha. Now, if we repeat this many times, we can plot the distribution of our estimates, and find its SD. We use the `boot` library to do this.

```{r}
library(boot)
stats = boot(Portfolio, alpha_estimator, R=1000)
plot(stats)
stats
```

In the plot, the distribution of our estimates looks approximately normal. The output tells us our prediction for alpha = 0.5758, with standard deviation 0.0886.

### Estimating the accuracy of a linear regression model.

Here we use the bootstrap to analyze the variability of the coefficients in the linear regression model of `mpg~horswepower`.

```{r}
# Returns coefficients of linear regression model
boot_fn = function(data, index) {
  return(coef(lm(mpg~horsepower, data=data, subset=index)))
}

# Use all 392 observations to obtain coefficients
boot_fn(Auto, 1:392)
```

```{r}
# Now use boot() to generate standard error estimates for 1000 estimates of the two coefficients
boot(Auto, boot_fn, R=1000)
```

So, the SE of the intercept term is 0.85 and that of the slope term is 0.007. How do these compare to the SEs of just one regression using all the data?

```{r}
summary(lm(mpg~horsepower, data=Auto))
```

So, the SE comparisons are the following, in order (regression summary SE, bootstrap estimate) for intercept and then slope: (0.717, 0.847) and (0.006, 0.007). These are different. Why? The standard formulas given on p. 66 rely on several assumptions. For example, they depend on the unknown noise variance, estimated using the RSS. The estimate of this parameter depends on the linear model being correct, which we know isn't (we've seen the quadratic fit is better). So the residuals from a linear fit will be inflated, and so will this variance parameter. And, the standard formulas assume that the observations are fixed, and all the variability comes from the variation in the true errors between the predicted values and the population regression line. The bootstrap does not depend on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors than summary().

With this in mind, let's run a bootstrap to see what the variability in the SEs is if we instead choose the quadratic model...

```{r}
# modify boot_fn to this time be a polynomial regression of degree 2
boot_fn_2 = function(data, index) {
  return(coef(lm(mpg~poly(horsepower, 2), data=data, subset=index)))
}

# find bootstrap SE estimates
boot(Auto, boot_fn_2, R=1000)

# compare with what summary() tells us
summary(lm(mpg~poly(horsepower, 2), data=Auto))
```

Now, we compare the following: (0.2209, 0.2156), (4.3739, 3.6796), and (4.3739, 4.3662). These are more similar, which makes sense given we used the better model this time!